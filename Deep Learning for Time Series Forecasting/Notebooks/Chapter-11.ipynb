{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Develop Simple Methods for Univariate Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-step naive forecast\n",
    "def naive_forecast(history, n):\n",
    "    return history[-n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
      "100\n",
      "90\n",
      "80\n",
      "70\n",
      "60\n",
      "50\n",
      "40\n",
      "30\n",
      "20\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# define dataset\n",
    "data = [10,20,30,40,50,60,70,80,90,100]\n",
    "print(data)\n",
    "# test naive forecast\n",
    "for i in range(1, len(data)+1):\n",
    "    print(naive_forecast(data,i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:6: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_4892\\2103709765.py:6: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if avg_type is 'mean':\n"
     ]
    }
   ],
   "source": [
    "from numpy import mean, median\n",
    "# one-step average forecast\n",
    "def average_forecast(history: list, config: tuple):\n",
    "    n, avg_type = config\n",
    "    # mean of last n values\n",
    "    if avg_type is 'mean':\n",
    "        return mean(history[-n:])\n",
    "    # median of last n values\n",
    "    return median(history[-n:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.0, 20.0, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0, 90.0, 100.0]\n",
      "55.0\n"
     ]
    }
   ],
   "source": [
    "# define dataset\n",
    "data = [10.0, 20.0, 30.0, 40.0, 50.0, 60.0, 70.0, 80.0, 90.0, 100.0]\n",
    "print(data)\n",
    "# test naive forecast\n",
    "for i in range(i, len(data)+1):\n",
    "    print(average_forecast(data, (i, 'mean')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can update the function to support averaging over seasonal data, respecting the seasonal offset. An offset argument can be added to the function that when not set to 1 will determine the number of prior observations backwards to count before collecting values from which to include the average. For example, if $\\footnotesize n = 1 $, and $\\footnotesize offset = 3$, then the average is calculated from the single value at $ \\footnotesize n \\times offset$  or $ \\footnotesize 1 \\times 3 = -3 $. If $\\footnotesize n = 2$ and $\\footnotesize offset = 3$, then the average is calculated from the values at $\\footnotesize 1 \\times 3$ or $\\footnotesize-3$ and $\\footnotesize 2 \\times 3$ or $\\footnotesize 6$.\n",
    "\n",
    "We can also add some protection to raise an exception when seasonal configuration (n x offset) extends beyond the end of the historical observationts. \n",
    "\n",
    "The updated function is listed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:19: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:19: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_10492\\4284339597.py:19: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if avg_type is 'mean':\n"
     ]
    }
   ],
   "source": [
    "# example of an average forecast for seasonal data\n",
    "from numpy import mean\n",
    "from numpy import median\n",
    "\n",
    "# one-step average foracast\n",
    "def average_forecast(history, config):\n",
    "    n, offset, avg_type = config\n",
    "    values = list()\n",
    "    if offset == 1:\n",
    "        values = history[-n:]\n",
    "    else:\n",
    "        if n*offset > len(history):\n",
    "            raise Exception('Config beyond end of data: %d %d' %(n, offset))\n",
    "        # try to collect n values using offset\n",
    "        for i in range(1, n+1):\n",
    "            ix = i*offset\n",
    "            values.append(history[-ix])\n",
    "    # mean of last n values\n",
    "    if avg_type is 'mean':\n",
    "        return mean(values)\n",
    "    # median of last n values\n",
    "    return median(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.0, 20.0, 30.0, 10.0, 20.0, 30.0, 10.0, 20.0, 30.0]\n",
      "10.0\n",
      "10.0\n",
      "10.0\n"
     ]
    }
   ],
   "source": [
    "# define dataset\n",
    "data = [10.0, 20.0, 30.0, 10.0, 20.0, 30.0, 10.0, 20.0, 30.0]\n",
    "print(data)\n",
    "# test naive forecast\n",
    "for i in [1, 2, 3]:\n",
    "    print(average_forecast(data, (i, 3, 'mean')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to combine both the naive and the average forecast strategies together into the same function. There is a little overlap between the methods, specifically the n-offset into the history that is used to either persist values or determine the number of values to average.\n",
    "\n",
    "It is helpful to have both strategies supported by one function so that we can test a suite of conffigurations for both strategies at once as part of a broader grid search of simple models. The simple forecast() function below combines both strategies into a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-step simple forecast\n",
    "def simple_forecast(history, config):\n",
    "    n, offset, avg_type = config\n",
    "    # persist value, ignore other config\n",
    "    if avg_type == 'persist':\n",
    "        return history[-n]\n",
    "    # collect values to average\n",
    "    values = list()\n",
    "    if offset == 1:\n",
    "        values = history[-n:]\n",
    "    else:\n",
    "        # skip bad configs\n",
    "        if n*offset > len(history):\n",
    "            raise Exception('Config beyond end of data: %d %d' % (n,offset))\n",
    "        # try and collect n values using offset\n",
    "        for i in range(1, n+1):\n",
    "            ix = i * offset\n",
    "        values.append(history[-ix])\n",
    "    # check if we can average\n",
    "    if len(values) < 2:\n",
    "        raise Exception('Cannot calculate average')\n",
    "    # mean of last n values\n",
    "    if avg_type == 'mean':\n",
    "        return mean(values)\n",
    "    # median of last n values\n",
    "    return median(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to build up some functions for fitting and evaluating a model repeatedly via walk-forward validation, including splitting a dataset into train and test sets and evaluating one-step forecasts. We can split a list or NumPy array of data using a slice given a specfied size of the split, e.g. the number of time steps to use from the data in the test set. The train test split() function below implements this for a provided dataset and a specfied number of time steps to use in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a univariate dataset into train/test sets\n",
    "def train_test_split(data, n_test):\n",
    "    return data[:-n_test], data[-n_test:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# root mean squared error or rmse\n",
    "def measure_rmse(actual, predicted):\n",
    "    return sqrt(mean_squared_error(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# walk-forward validation for univariate data\n",
    "def walk_forward_validation(data, n_test, cfg):\n",
    "    predictions = list()\n",
    "    # split dataset\n",
    "    train, test = train_test_split(data, n_test)\n",
    "    # seed history with training dataset\n",
    "    history = [x for x in train]\n",
    "    # step over each time step in the test set\n",
    "    for i in range(len(test)):\n",
    "        # fit model and make forecast for history\n",
    "        yhat = simple_forecast(history, cfg)\n",
    "        # store forecast in list of predictions\n",
    "        predictions.append(yhat)\n",
    "        # add actual observation to history for the next loop\n",
    "        history.append(test[i])\n",
    "    # estimate prediction error\n",
    "    error = measure_rmse(test, predictions)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import catch_warnings\n",
    "from warnings import filterwarnings\n",
    "# score a model, return None on failure\n",
    "def score_model(data, n_test, cfg, debug=False):\n",
    "    result = None\n",
    "    # convert config to a key\n",
    "    key = str(cfg)\n",
    "    # show all warnings and fail on exception if debugging\n",
    "    if debug:\n",
    "        result = walk_forward_validation(data, n_test, cfg)\n",
    "    else:\n",
    "    # one failure during model validation suggests an unstable config\n",
    "        try:\n",
    "            # never show warnings when grid searching, too noisy\n",
    "            with catch_warnings():\n",
    "                filterwarnings(\"ignore\")\n",
    "                result = walk_forward_validation(data, n_test, cfg)\n",
    "        except:\n",
    "            error = None\n",
    "    # check for an interesting result\n",
    "    if result is not None:\n",
    "        print(' > Model[%s] %.3f' % (key, result))\n",
    "    return (key, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel\n",
    "from multiprocessing import cpu_count\n",
    "# define executor\n",
    "executor = Parallel(n_jobs=cpu_count(), backend='multiprocessing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
