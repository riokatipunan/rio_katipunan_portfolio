{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Grid Search Deep Learning Models for Univariate Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search persistence models for monthly airline passengers dataset\n",
    "from math import sqrt\n",
    "from numpy import mean\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a univariate dataset into train/test sets\n",
    "def train_test_split(data, n_test):\n",
    "    return data[:-n_test], data[-n_test:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root mean squared error or rmse\n",
    "def measure_rmse(actual, predicted):\n",
    "    return sqrt(mean_squared_error(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model\n",
    "def model_fit(train, config):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast with a pre-fit model\n",
    "def model_predict(model, history, offset):\n",
    "    return history[-offset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# walk-forward validation for univariate data\n",
    "def walk_forward_validation(data, n_test, cfg):\n",
    "    predictions = list()\n",
    "    # split dataset\n",
    "    train, test = train_test_split(data, n_test)\n",
    "    # fit model\n",
    "    model = model_fit(train, cfg)\n",
    "    # seed history with training dataset\n",
    "    history = [x for x in train]\n",
    "    # step over each time-step in the test set\n",
    "    for i in range(len(test)):\n",
    "        # fit model and make forecast for history\n",
    "        yhat = model_predict(model, history, cfg)\n",
    "        # store forecast in list of predictions\n",
    "        predictions.append(yhat)\n",
    "        # add actual observation to history for the next loop\n",
    "        history.append(test[i])\n",
    "    # estimate prediction error\n",
    "    error = measure_rmse(test, predictions)\n",
    "    print(' > %.3f' % error)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score a model, return None on failure\n",
    "def repeat_evaluate(data, config, n_test, n_repeats=10):\n",
    "    # convert config to a key\n",
    "    key = str(config)\n",
    "    # fit and evaluate the model n times\n",
    "    scores = [walk_forward_validation(data, n_test, config) for _ in range(n_repeats)]\n",
    "    # summarize score\n",
    "    result = mean(scores)\n",
    "    print('> Model[%s] %.3f' % (key, result))\n",
    "    return (key, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search configs\n",
    "def grid_search(data, cfg_list, n_test):\n",
    "    # evaluate configs\n",
    "    scores = scores = [repeat_evaluate(data, cfg, n_test) for cfg in cfg_list]\n",
    "    # sort configs by error, asc\n",
    "    scores.sort(key=lambda tup: tup[1])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > 53.152\n",
      " > 53.152\n",
      " > 53.152\n",
      " > 53.152\n",
      " > 53.152\n",
      " > 53.152\n",
      " > 53.152\n",
      " > 53.152\n",
      " > 53.152\n",
      " > 53.152\n",
      "> Model[1] 53.152\n",
      " > 126.735\n",
      " > 126.735\n",
      " > 126.735\n",
      " > 126.735\n",
      " > 126.735\n",
      " > 126.735\n",
      " > 126.735\n",
      " > 126.735\n",
      " > 126.735\n",
      " > 126.735\n",
      "> Model[6] 126.735\n",
      " > 50.708\n",
      " > 50.708\n",
      " > 50.708\n",
      " > 50.708\n",
      " > 50.708\n",
      " > 50.708\n",
      " > 50.708\n",
      " > 50.708\n",
      " > 50.708\n",
      " > 50.708\n",
      "> Model[12] 50.708\n",
      " > 97.110\n",
      " > 97.110\n",
      " > 97.110\n",
      " > 97.110\n",
      " > 97.110\n",
      " > 97.110\n",
      " > 97.110\n",
      " > 97.110\n",
      " > 97.110\n",
      " > 97.110\n",
      "> Model[24] 97.110\n",
      " > 110.274\n",
      " > 110.274\n",
      " > 110.274\n",
      " > 110.274\n",
      " > 110.274\n",
      " > 110.274\n",
      " > 110.274\n",
      " > 110.274\n",
      " > 110.274\n",
      " > 110.274\n",
      "> Model[36] 110.274\n",
      "done\n",
      "12 50.708316214732804\n",
      "1 53.1515129919491\n",
      "24 97.10990337413241\n",
      "36 110.27352356753639\n",
      "6 126.73495965991387\n"
     ]
    }
   ],
   "source": [
    "# define dataset\n",
    "series = read_csv('../Data/Chapter 15/airline-passengers.csv', header=0, index_col=0)\n",
    "data = series.values\n",
    "# data split\n",
    "n_test = 12\n",
    "# model configs\n",
    "cfg_list = [1, 6, 12, 24, 36]\n",
    "# grid search\n",
    "scores = grid_search(data, cfg_list, n_test)\n",
    "print('done')\n",
    "# list top 10 configs\n",
    "for cfg, error in scores[:10]:\n",
    "    print(cfg, error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search mlps for monthly airline passengers dataset\n",
    "from math import sqrt\n",
    "from numpy import array\n",
    "from numpy import mean\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a univariate dataset into train/test sets\n",
    "def train_test_split(data, n_test):\n",
    "    return data[:-n_test], data[-n_test:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform list into supervised learning format\n",
    "def series_to_supervised(data, n_in, n_out=1):\n",
    "    df = DataFrame(data)\n",
    "    cols = list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    # drop rows with NaN values\n",
    "    agg.dropna(inplace=True)\n",
    "    return agg.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root mean squared error or rmse\n",
    "def measure_rmse(actual, predicted):\n",
    "    return sqrt(mean_squared_error(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference dataset\n",
    "def difference(data, order):\n",
    "    return [data[i] - data[i - order] for i in range(order, len(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model\n",
    "def model_fit(train, config):\n",
    "    # unpack config\n",
    "    n_input, n_nodes, n_epochs, n_batch, n_diff = config\n",
    "    # prepare data\n",
    "    if n_diff > 0:\n",
    "        train = difference(train, n_diff)\n",
    "    # transform series into supervised format\n",
    "    data = series_to_supervised(train, n_in=n_input)\n",
    "    # separate inputs and outputs\n",
    "    train_x, train_y = data[:, :-1], data[:, -1]\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_nodes, activation='relu', input_dim=n_input))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    # fit model\n",
    "    model.fit(train_x, train_y, epochs=n_epochs, batch_size=n_batch, verbose=0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast with the fit model\n",
    "def model_predict(model, history, config):\n",
    "    # unpack config\n",
    "    n_input, _, _, _, n_diff = config\n",
    "    # prepare data\n",
    "    correction = 0.0\n",
    "    if n_diff > 0:\n",
    "        correction = history[-n_diff]\n",
    "        history = difference(history, n_diff)\n",
    "    # shape input for model\n",
    "    x_input = array(history[-n_input:]).reshape((1, n_input))\n",
    "    # make forecast\n",
    "    yhat = model.predict(x_input, verbose=0)\n",
    "    # correct forecast if it was differenced\n",
    "    return correction + yhat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# walk-forward validation for univariate data\n",
    "def walk_forward_validation(data, n_test, cfg):\n",
    "    predictions = list()\n",
    "    # split dataset\n",
    "    train, test = train_test_split(data, n_test)\n",
    "    # fit model\n",
    "    model = model_fit(train, cfg)\n",
    "    # seed history with training dataset\n",
    "    history = [x for x in train]\n",
    "    # step over each time-step in the test set\n",
    "    for i in range(len(test)):\n",
    "        # fit model and make forecast for history\n",
    "        yhat = model_predict(model, history, cfg)\n",
    "        # store forecast in list of predictions\n",
    "        predictions.append(yhat)\n",
    "        # add actual observation to history for the next loop\n",
    "        history.append(test[i])\n",
    "    # estimate prediction error\n",
    "    error = measure_rmse(test, predictions)\n",
    "    print(' > %.3f' % error)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score a model, return None on failure\n",
    "def repeat_evaluate(data, config, n_test, n_repeats=10):\n",
    "    # convert config to a key\n",
    "    key = str(config)\n",
    "    # fit and evaluate the model n times\n",
    "    scores = [walk_forward_validation(data, n_test, config) for _ in range(n_repeats)]\n",
    "    # summarize score\n",
    "    result = mean(scores)\n",
    "    print('> Model[%s] %.3f' % (key, result))\n",
    "    return (key, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search configs\n",
    "def grid_search(data, cfg_list, n_test):\n",
    "    # evaluate configs\n",
    "    scores = scores = [repeat_evaluate(data, cfg, n_test) for cfg in cfg_list]\n",
    "    # sort configs by error, asc\n",
    "    scores.sort(key=lambda tup: tup[1])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of configs to try\n",
    "def model_configs():\n",
    "    # define scope of configs\n",
    "    n_input = [12]\n",
    "    n_nodes = [50, 100]\n",
    "    n_epochs = [100]\n",
    "    n_batch = [1, 150]\n",
    "    n_diff = [0, 12]\n",
    "    # create configs\n",
    "    configs = list()\n",
    "    for i in n_input:\n",
    "        for j in n_nodes:\n",
    "            for k in n_epochs:\n",
    "                for l in n_batch:\n",
    "                    for m in n_diff:\n",
    "                        cfg = [i, j, k, l, m]\n",
    "                        configs.append(cfg)\n",
    "    print('Total configs: %d' % len(configs))\n",
    "    return configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total configs: 8\n",
      " > 18.406\n",
      " > 24.299\n",
      " > 18.984\n",
      " > 16.674\n",
      " > 28.954\n",
      " > 26.365\n",
      " > 18.961\n",
      " > 23.128\n",
      " > 19.087\n",
      " > 25.964\n",
      "> Model[[12, 50, 100, 1, 0]] 22.082\n",
      " > 17.681\n",
      " > 19.267\n",
      " > 19.385\n",
      " > 22.197\n",
      " > 21.615\n",
      " > 19.544\n",
      " > 19.162\n",
      " > 24.026\n",
      " > 20.030\n",
      " > 19.088\n",
      "> Model[[12, 50, 100, 1, 12]] 20.200\n",
      " > 33.526\n",
      " > 83.337\n",
      " > 43.550\n",
      " > 20.876\n",
      " > 77.279\n",
      " > 46.261\n",
      " > 48.457\n",
      " > 36.287\n",
      " > 57.230\n",
      " > 70.814\n",
      "> Model[[12, 50, 100, 150, 0]] 51.762\n",
      " > 20.499\n",
      " > 19.962\n",
      " > 20.932\n",
      " > 19.200\n",
      " > 20.945\n",
      " > 19.413\n",
      " > 20.486\n",
      " > 18.449\n",
      " > 20.728\n",
      " > 19.289\n",
      "> Model[[12, 50, 100, 150, 12]] 19.990\n",
      " > 18.130\n",
      " > 17.011\n",
      " > 20.065\n",
      " > 20.200\n",
      " > 21.118\n",
      " > 27.547\n",
      " > 19.931\n",
      " > 21.088\n",
      " > 16.946\n",
      " > 34.465\n",
      "> Model[[12, 100, 100, 1, 0]] 21.650\n",
      " > 19.456\n",
      " > 18.505\n",
      " > 18.995\n",
      " > 19.771\n",
      " > 19.100\n",
      " > 18.731\n",
      " > 19.701\n",
      " > 17.761\n",
      " > 19.632\n",
      " > 21.152\n",
      "> Model[[12, 100, 100, 1, 12]] 19.281\n",
      " > 70.029\n",
      " > 67.631\n",
      " > 44.022\n",
      " > 29.904\n",
      " > 35.236\n",
      " > 35.846\n",
      " > 51.924\n",
      " > 64.686\n",
      " > 48.535\n",
      " > 73.336\n",
      "> Model[[12, 100, 100, 150, 0]] 52.115\n",
      " > 19.639\n",
      " > 19.518\n",
      " > 18.838\n",
      " > 19.832\n",
      " > 18.376\n",
      " > 19.266\n",
      " > 19.932\n",
      " > 20.096\n",
      " > 18.887\n",
      " > 19.734\n",
      "> Model[[12, 100, 100, 150, 12]] 19.412\n",
      "done\n",
      "[12, 100, 100, 1, 12] 19.28058817443042\n",
      "[12, 100, 100, 150, 12] 19.411836281557697\n",
      "[12, 50, 100, 150, 12] 19.990318540603386\n"
     ]
    }
   ],
   "source": [
    "# define dataset\n",
    "series = read_csv('../Data/Chapter 15/airline-passengers.csv', header=0, index_col=0)\n",
    "data = series.values\n",
    "# data split\n",
    "n_test = 12\n",
    "# model configs\n",
    "cfg_list = model_configs()\n",
    "# grid search\n",
    "scores = grid_search(data, cfg_list, n_test)\n",
    "print('done')\n",
    "# list top 3 configs\n",
    "for cfg, error in scores[:3]:\n",
    "    print(cfg, error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search cnn for monthly airline passengers dataset\n",
    "from math import sqrt\n",
    "from numpy import array\n",
    "from numpy import mean\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a univariate dataset into train/test sets\n",
    "def train_test_split(data, n_test):\n",
    "    return data[:-n_test], data[-n_test:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform list into supervised learning format\n",
    "def series_to_supervised(data, n_in, n_out=1):\n",
    "    df = DataFrame(data)\n",
    "    cols = list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    # drop rows with NaN values\n",
    "    agg.dropna(inplace=True)\n",
    "    return agg.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root mean squared error or rmse\n",
    "def measure_rmse(actual, predicted):\n",
    "    return sqrt(mean_squared_error(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference dataset\n",
    "def difference(data, order):\n",
    "    return [data[i] - data[i - order] for i in range(order, len(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model\n",
    "def model_fit(train, config):\n",
    "    # unpack config\n",
    "    n_input, n_filters, n_kernel, n_epochs, n_batch, n_diff = config\n",
    "    # prepare data\n",
    "    if n_diff > 0:\n",
    "        train = difference(train, n_diff)\n",
    "    # transform series into supervised format\n",
    "    data = series_to_supervised(train, n_in=n_input)\n",
    "    # separate inputs and outputs\n",
    "    train_x, train_y = data[:, :-1], data[:, -1]\n",
    "    # reshape input data into [samples, timesteps, features]\n",
    "    n_features = 1\n",
    "    train_x = train_x.reshape((train_x.shape[0], train_x.shape[1], n_features))\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=n_filters, kernel_size=n_kernel, activation='relu',\n",
    "        input_shape=(n_input, n_features)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    # fit\n",
    "    model.fit(train_x, train_y, epochs=n_epochs, batch_size=n_batch, verbose=0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast with the fit model\n",
    "def model_predict(model, history, config):\n",
    "    # unpack config\n",
    "    n_input, _, _, _, _, n_diff = config\n",
    "    # prepare data\n",
    "    correction = 0.0\n",
    "    if n_diff > 0:\n",
    "        correction = history[-n_diff]\n",
    "        history = difference(history, n_diff)\n",
    "    x_input = array(history[-n_input:]).reshape((1, n_input, 1))\n",
    "    # forecast\n",
    "    yhat = model.predict(x_input, verbose=0)\n",
    "    return correction + yhat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# walk-forward validation for univariate data\n",
    "def walk_forward_validation(data, n_test, cfg):\n",
    "    predictions = list()\n",
    "    # split dataset\n",
    "    train, test = train_test_split(data, n_test)\n",
    "    # fit model\n",
    "    model = model_fit(train, cfg)\n",
    "    # seed history with training dataset\n",
    "    history = [x for x in train]\n",
    "    # step over each time-step in the test set\n",
    "    for i in range(len(test)):\n",
    "        # fit model and make forecast for history\n",
    "        yhat = model_predict(model, history, cfg)\n",
    "        # store forecast in list of predictions\n",
    "        predictions.append(yhat)\n",
    "        # add actual observation to history for the next loop\n",
    "        history.append(test[i])\n",
    "    # estimate prediction error\n",
    "    error = measure_rmse(test, predictions)\n",
    "    print(' > %.3f' % error)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score a model, return None on failure\n",
    "def repeat_evaluate(data, config, n_test, n_repeats=10):\n",
    "    # convert config to a key\n",
    "    key = str(config)\n",
    "    # fit and evaluate the model n times\n",
    "    scores = [walk_forward_validation(data, n_test, config) for _ in range(n_repeats)]\n",
    "    # summarize score\n",
    "    result = mean(scores)\n",
    "    print('> Model[%s] %.3f' % (key, result))\n",
    "    return (key, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search configs\n",
    "def grid_search(data, cfg_list, n_test):\n",
    "    # evaluate configs\n",
    "    scores = scores = [repeat_evaluate(data, cfg, n_test) for cfg in cfg_list]\n",
    "    # sort configs by error, asc\n",
    "    scores.sort(key=lambda tup: tup[1])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of configs to try\n",
    "def model_configs():\n",
    "    # define scope of configs\n",
    "    n_input = [12]\n",
    "    n_filters = [64]\n",
    "    n_kernels = [3, 5]\n",
    "    n_epochs = [100]\n",
    "    n_batch = [1, 150]\n",
    "    n_diff = [0, 12]\n",
    "    # create configs\n",
    "    configs = list()\n",
    "    for a in n_input:\n",
    "        for b in n_filters:\n",
    "            for c in n_kernels:\n",
    "                for d in n_epochs:\n",
    "                    for e in n_batch:\n",
    "                        for f in n_diff:\n",
    "                            cfg = [a,b,c,d,e,f]\n",
    "                            configs.append(cfg)\n",
    "    print('Total configs: %d' % len(configs))\n",
    "    return configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total configs: 8\n",
      " > 21.191\n",
      " > 21.676\n",
      " > 18.337\n",
      " > 22.913\n",
      " > 19.789\n",
      " > 18.116\n",
      " > 18.721\n",
      " > 22.927\n",
      " > 22.572\n",
      " > 25.597\n",
      "> Model[[12, 64, 3, 100, 1, 0]] 21.184\n",
      " > 18.626\n",
      " > 20.948\n",
      " > 20.037\n",
      " > 20.068\n",
      " > 19.703\n",
      " > 18.391\n",
      " > 20.342\n",
      " > 22.138\n",
      " > 21.747\n",
      " > 18.440\n",
      "> Model[[12, 64, 3, 100, 1, 12]] 20.044\n",
      " > 76.270\n",
      " > 79.092\n",
      " > 74.395\n",
      " > 77.784\n",
      " > 82.835\n",
      " > 79.876\n",
      " > 80.606\n",
      " > 77.009\n",
      " > 79.555\n",
      " > 84.672\n",
      "> Model[[12, 64, 3, 100, 150, 0]] 79.210\n",
      " > 18.491\n",
      " > 18.880\n",
      " > 19.240\n",
      " > 18.105\n",
      " > 19.260\n",
      " > 19.824\n",
      " > 19.400\n",
      " > 19.218\n",
      " > 20.373\n",
      " > 18.588\n",
      "> Model[[12, 64, 3, 100, 150, 12]] 19.138\n",
      " > 23.578\n",
      " > 23.436\n",
      " > 19.735\n",
      " > 19.258\n",
      " > 21.578\n",
      " > 23.073\n",
      " > 19.086\n",
      " > 18.683\n",
      " > 19.557\n",
      " > 25.783\n",
      "> Model[[12, 64, 5, 100, 1, 0]] 21.377\n",
      " > 17.371\n",
      " > 18.170\n",
      " > 19.451\n",
      " > 19.739\n",
      " > 19.339\n",
      " > 19.916\n",
      " > 18.785\n",
      " > 19.196\n",
      " > 19.513\n",
      " > 18.286\n",
      "> Model[[12, 64, 5, 100, 1, 12]] 18.977\n",
      " > 78.589\n",
      " > 77.999\n",
      " > 86.359\n",
      " > 91.132\n",
      " > 75.211\n",
      " > 77.382\n",
      " > 86.906\n",
      " > 84.784\n",
      " > 66.654\n",
      " > 72.443\n",
      "> Model[[12, 64, 5, 100, 150, 0]] 79.746\n",
      " > 18.793\n",
      " > 19.188\n",
      " > 19.229\n",
      " > 18.455\n",
      " > 20.794\n",
      " > 19.307\n",
      " > 18.779\n",
      " > 20.138\n",
      " > 21.428\n",
      " > 20.194\n",
      "> Model[[12, 64, 5, 100, 150, 12]] 19.631\n",
      "done\n",
      "[12, 64, 5, 100, 1, 12] 18.97674638294396\n",
      "[12, 64, 3, 100, 150, 12] 19.13790916582969\n",
      "[12, 64, 5, 100, 150, 12] 19.63057318450486\n"
     ]
    }
   ],
   "source": [
    "# define dataset\n",
    "series = read_csv('../Data/Chapter 15/airline-passengers.csv', header=0, index_col=0)\n",
    "data = series.values\n",
    "# data split\n",
    "n_test = 12\n",
    "# model configs\n",
    "cfg_list = model_configs()\n",
    "# grid search\n",
    "scores = grid_search(data, cfg_list, n_test)\n",
    "print('done')\n",
    "# list top 10 configs\n",
    "for cfg, error in scores[:3]:\n",
    "    print(cfg, error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short Term Memory Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search lstm for monthly airline passengers dataset\n",
    "from math import sqrt\n",
    "from numpy import array\n",
    "from numpy import mean\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a univariate dataset into train/test sets\n",
    "def train_test_split(data, n_test):\n",
    "    return data[:-n_test], data[-n_test:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform list into supervised learning format\n",
    "def series_to_supervised(data, n_in, n_out=1):\n",
    "    df = DataFrame(data)\n",
    "    cols = list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    # drop rows with NaN values\n",
    "    agg.dropna(inplace=True)\n",
    "    return agg.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root mean squared error or rmse\n",
    "def measure_rmse(actual, predicted):\n",
    "    return sqrt(mean_squared_error(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference dataset\n",
    "def difference(data, order):\n",
    "    return [data[i] - data[i - order] for i in range(order, len(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model\n",
    "def model_fit(train, config):\n",
    "    # unpack config\n",
    "    n_input, n_nodes, n_epochs, n_batch, n_diff = config\n",
    "    # prepare data\n",
    "    if n_diff > 0:\n",
    "        train = difference(train, n_diff)\n",
    "    # transform series into supervised format\n",
    "    data = series_to_supervised(train, n_in=n_input)\n",
    "    # separate inputs and outputs\n",
    "    train_x, train_y = data[:, :-1], data[:, -1]\n",
    "    # reshape input data into [samples, timesteps, features]\n",
    "    n_features = 1\n",
    "    train_x = train_x.reshape((train_x.shape[0], train_x.shape[1], n_features))\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_nodes, activation='relu', input_shape=(n_input, n_features)))\n",
    "    model.add(Dense(n_nodes, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    # fit model\n",
    "    model.fit(train_x, train_y, epochs=n_epochs, batch_size=n_batch, verbose=0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast with the fit model\n",
    "def model_predict(model, history, config):\n",
    "    # unpack config\n",
    "    n_input, _, _, _, n_diff = config\n",
    "    # prepare data\n",
    "    correction = 0.0\n",
    "    if n_diff > 0:\n",
    "        correction = history[-n_diff]\n",
    "        history = difference(history, n_diff)\n",
    "    # reshape sample into [samples, timesteps, features]\n",
    "    x_input = array(history[-n_input:]).reshape((1, n_input, 1))\n",
    "    # forecast\n",
    "    yhat = model.predict(x_input, verbose=0)\n",
    "    return correction + yhat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# walk-forward validation for univariate data\n",
    "def walk_forward_validation(data, n_test, cfg):\n",
    "    predictions = list()\n",
    "    # split dataset\n",
    "    train, test = train_test_split(data, n_test)\n",
    "    # fit model\n",
    "    model = model_fit(train, cfg)\n",
    "    # seed history with training dataset\n",
    "    history = [x for x in train]\n",
    "    # step over each time-step in the test set\n",
    "    for i in range(len(test)):\n",
    "        # fit model and make forecast for history\n",
    "        yhat = model_predict(model, history, cfg)\n",
    "        # store forecast in list of predictions\n",
    "        predictions.append(yhat)\n",
    "        # add actual observation to history for the next loop\n",
    "        history.append(test[i])\n",
    "    # estimate prediction error\n",
    "    error = measure_rmse(test, predictions)\n",
    "    print(' > %.3f' % error)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score a model, return None on failure\n",
    "def repeat_evaluate(data, config, n_test, n_repeats=10):\n",
    "    # convert config to a key\n",
    "    key = str(config)\n",
    "    # fit and evaluate the model n times\n",
    "    scores = [walk_forward_validation(data, n_test, config) for _ in range(n_repeats)]\n",
    "    # summarize score\n",
    "    result = mean(scores)\n",
    "    print('> Model[%s] %.3f' % (key, result))\n",
    "    return (key, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search configs\n",
    "def grid_search(data, cfg_list, n_test):\n",
    "    # evaluate configs\n",
    "    scores = scores = [repeat_evaluate(data, cfg, n_test) for cfg in cfg_list]\n",
    "    # sort configs by error, asc\n",
    "    scores.sort(key=lambda tup: tup[1])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of configs to try\n",
    "def model_configs():\n",
    "    # define scope of configs\n",
    "    n_input = [12]\n",
    "    n_nodes = [100]\n",
    "    n_epochs = [50]\n",
    "    n_batch = [1, 150]\n",
    "    n_diff = [12]\n",
    "    # create configs\n",
    "    configs = list()\n",
    "    for i in n_input:\n",
    "        for j in n_nodes:\n",
    "            for k in n_epochs:\n",
    "                for l in n_batch:\n",
    "                    for m in n_diff:\n",
    "                        cfg = [i, j, k, l, m]\n",
    "                        configs.append(cfg)\n",
    "    print('Total configs: %d' % len(configs))\n",
    "    return configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total configs: 2\n",
      " > 19.783\n",
      " > 25.841\n",
      " > 24.664\n",
      " > 21.850\n",
      " > 29.665\n",
      " > 22.341\n",
      " > 23.826\n",
      " > 25.768\n",
      " > 29.976\n",
      " > 25.112\n",
      "> Model[[12, 100, 50, 1, 12]] 24.883\n",
      " > 27.636\n",
      " > 19.161\n",
      " > 21.991\n",
      " > 19.009\n",
      " > 25.502\n",
      " > 21.308\n",
      " > 23.992\n",
      " > 23.741\n",
      " > 26.131\n",
      " > 21.310\n",
      "> Model[[12, 100, 50, 150, 12]] 22.978\n",
      "done\n",
      "[12, 100, 50, 150, 12] 22.978132402195616\n",
      "[12, 100, 50, 1, 12] 24.882554915946677\n"
     ]
    }
   ],
   "source": [
    "# define dataset\n",
    "series = read_csv('../Data/Chapter 15/airline-passengers.csv', header=0, index_col=0)\n",
    "data = series.values\n",
    "# data split\n",
    "n_test = 12\n",
    "# model configs\n",
    "cfg_list = model_configs()\n",
    "# grid search\n",
    "scores = grid_search(data, cfg_list, n_test)\n",
    "print('done')\n",
    "# list top 10 configs\n",
    "for cfg, error in scores[:3]:\n",
    "    print(cfg, error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
